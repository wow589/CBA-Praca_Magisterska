{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c4b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarc import CBA, TransactionDB\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import random\n",
    "import fim\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import time\n",
    "from pyarc.algorithms import (\n",
    "    top_rules,\n",
    "    createCARs,\n",
    "    generateCARs,\n",
    "    M1Algorithm,\n",
    "    M2Algorithm,\n",
    "    Classifier\n",
    ")\n",
    "from rich.progress import (Progress, SpinnerColumn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103012b6",
   "metadata": {},
   "source": [
    "<h1>Functions definitions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8f61f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validate(dataset, min_sup, min_conf, folds=10):\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=6)\n",
    "    accuracy = []\n",
    "    data = dataset.dataframe\n",
    "    split = kf.split(data)\n",
    "\n",
    "    for train_indices, test_indices in split:\n",
    "        train_data = data.iloc[train_indices]\n",
    "        test_data = data.iloc[test_indices]\n",
    "        #handling .csv when target column isn't the last one\n",
    "        if(dataset.target != \"\"):\n",
    "            txns_train = TransactionDB.from_DataFrame(train_data, target=dataset.target)\n",
    "            txns_test = TransactionDB.from_DataFrame(test_data, target=dataset.target)\n",
    "        else:\n",
    "            txns_train = TransactionDB.from_DataFrame(train_data)\n",
    "            txns_test = TransactionDB.from_DataFrame(test_data)\n",
    "        appear = txns_train.appeardict\n",
    "        rules = fim.apriori(txns_train.string_representation, supp=min_sup, conf=min_conf, mode=\"o\", target=\"r\", report=\"sc\", appear=txns_train.appeardict, zmax=10)\n",
    "        cars = createCARs(rules)\n",
    "        clf = M1Algorithm(cars, txns_train).build()\n",
    "        accuracy.append(clf.test_transactions(txns_test))\n",
    "        \n",
    "    return accuracy, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5cb7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset is class which has pandas dataframe with handled nan values\n",
    "class Dataset:\n",
    "    def __init__(self, name, path, na_values = \"\", target = \"\"):\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        self.target = target\n",
    "        df = pd.read_csv(path, na_values=na_values)\n",
    "        self.dataframe = handle_nans(df)\n",
    "        self.min_rule_supp = 0\n",
    "        self.max_rule_supp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305ff671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing missing values with most common value in column\n",
    "def handle_nans(df):\n",
    "    if(df.isna().values.any()):\n",
    "        nan_cols = df.columns[df.isna().any()].tolist()\n",
    "        for col in nan_cols:\n",
    "            df[str(col)].fillna(df[str(col)].mode()[0], inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4cc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(datasets, min_sup, min_conf):\n",
    "        print(f\"Processing datasets with following config: Min_supp: {min_sup}%, Min_conf: {min_conf}%\")\n",
    "        dictionary = {\n",
    "            \"dataset_name\" : [],\n",
    "            \"rows_count\": [],\n",
    "            \"att_count\": [],\n",
    "            \"rules_count\": [],\n",
    "            \"min_rule_len\": [],\n",
    "            \"avg_rule_len\": [],\n",
    "            \"avg_rule_len_attr_ratio\": [],\n",
    "            \"max_rule_len\": [],\n",
    "            \"min_rule_supp\": [],\n",
    "            \"avg_rule_supp\": [],\n",
    "            \"max_rule_supp\": []\n",
    "        }\n",
    "\n",
    "        for dataset in datasets:\n",
    "            data = dataset.dataframe\n",
    "            \n",
    "            if(dataset.target != \"\"):\n",
    "                txns = TransactionDB.from_DataFrame(data, target=dataset.target)\n",
    "            else:\n",
    "                txns = TransactionDB.from_DataFrame(data)\n",
    "            rules = fim.apriori(txns.string_representation, supp=min_sup, conf=min_conf, mode=\"o\", target=\"r\", report=\"sc\", appear=txns.appeardict, zmax=10)\n",
    "            cars = createCARs(rules)\n",
    "            \n",
    "            dictionary[\"dataset_name\"].append(dataset.name)\n",
    "            dictionary[\"rows_count\"].append(len(data))\n",
    "            dictionary[\"att_count\"].append(data.shape[1]-1)\n",
    "            dictionary[\"rules_count\"].append(len(rules))\n",
    "            dictionary[\"min_rule_len\"].append(min(cars, key=lambda x: x.rulelen).rulelen)\n",
    "            dictionary[\"avg_rule_len\"].append(round((sum(c.rulelen for c in cars)/len(cars)), 2))#przez liczbe atrybutów\n",
    "            dictionary[\"avg_rule_len_attr_ratio\"].append(round((sum(c.rulelen for c in cars)/len(cars)/(data.shape[1]-1)), 2))#przez liczbe atrybutów\n",
    "            dictionary[\"max_rule_len\"].append(max(cars, key=lambda x: x.rulelen).rulelen)\n",
    "            \n",
    "            min_rule_supp = round(min(cars, key=lambda x: x.support).support * 100, 2)\n",
    "            dictionary[\"min_rule_supp\"].append(min_rule_supp)\n",
    "            dataset.min_rule_supp = min_rule_supp\n",
    "            \n",
    "            dictionary[\"avg_rule_supp\"].append(round((sum(c.support for c in cars)/len(cars))*100, 2))\n",
    "            \n",
    "            max_rule_supp = round(max(cars, key=lambda x: x.support).support * 100, 2)\n",
    "            dictionary[\"max_rule_supp\"].append(max_rule_supp)\n",
    "            dataset.max_rule_supp = max_rule_supp\n",
    "\n",
    "\n",
    "        datasets_df = pd.DataFrame(dictionary)\n",
    "        datasets_df = datasets_df[[\"dataset_name\", \"rows_count\", \"att_count\", \"rules_count\", \"min_rule_len\", \"avg_rule_len\", \"avg_rule_len_attr_ratio\", \"max_rule_len\", \"min_rule_supp\", \"avg_rule_supp\", \"max_rule_supp\"]]\n",
    "        \n",
    "        display(datasets_df)\n",
    "        return datasets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "590a34f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(datasets, min_sup, min_conf):\n",
    "        print(f\"Testing datasets with following config: Min_supp: {min_sup}%, Min_conf: {min_conf}%\")\n",
    "        dictionary = {\n",
    "            \"dataset_name\" : [],\n",
    "            \"accuracy [%]\": [],\n",
    "            \"std_deviation\": [],\n",
    "            \"execution_time\": []\n",
    "        }\n",
    "\n",
    "        for dataset in datasets:\n",
    "            start_time = time.time()\n",
    "            accuracy, clf = k_fold_cross_validate(dataset, min_sup, min_conf)\n",
    "            arr = np.array(accuracy, dtype='float32')\n",
    "            end_time = time.time()\n",
    "            dictionary[\"dataset_name\"].append(dataset.name)\n",
    "            dictionary[\"accuracy [%]\"].append(round(arr.mean()*100, 2))\n",
    "            dictionary[\"std_deviation\"].append(round(arr.std()*100, 2))\n",
    "            dictionary[\"execution_time\"].append(end_time - start_time)\n",
    "\n",
    "        datasets_df = pd.DataFrame(dictionary)\n",
    "        datasets_df = datasets_df[[\"dataset_name\", \"accuracy [%]\", \"std_deviation\", \"execution_time\"]]\n",
    "        return datasets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5af83c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_params(dataset):\n",
    "    dictionary = {\n",
    "            \"min_supp\" : [],\n",
    "            \"min_conf\": [],\n",
    "            \"clf_rules_count\": [],\n",
    "            \"accuracy\": [],\n",
    "            \"std\": []\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "    dataset_lower_bound = math.floor(dataset.min_rule_supp)\n",
    "    dataset_upper_bound = math.floor(dataset.max_rule_supp)+1\n",
    "    \n",
    "    progress = Progress(\n",
    "        SpinnerColumn(),\n",
    "        *Progress.get_default_columns()\n",
    "    )\n",
    "    \n",
    "    with progress as pb:\n",
    "        p1 = pb.add_task(f'Testing combinations for dataset: {dataset.name}', total=4*(dataset_upper_bound-dataset_lower_bound))\n",
    "        for c in range(20, 81, 20):\n",
    "            for s in range(dataset_lower_bound, dataset_upper_bound, 1):\n",
    "                step += 1\n",
    "                pb.update(task_id=p1, completed=step)\n",
    "                accuracy, clf = k_fold_cross_validate(dataset, s, c)\n",
    "                arr = np.array(accuracy, dtype='float32')\n",
    "                dictionary[\"min_supp\"].append(s)\n",
    "                dictionary[\"min_conf\"].append(c)\n",
    "                dictionary[\"clf_rules_count\"].append(clf.inspect()['lhs'].count())\n",
    "                dictionary[\"accuracy\"].append(round(arr.mean()*100, 2))\n",
    "                dictionary[\"std\"].append(round(arr.std()*100, 2))\n",
    "    end_time = time.time()\n",
    "    lapsed_time = end_time - start_time\n",
    "    \n",
    "    print(\"Execution time: {:.2f}s\".format(lapsed_time))\n",
    "    datasets_df = pd.DataFrame(dictionary)\n",
    "    datasets_df = datasets_df[[\"min_supp\", \"min_conf\", \"clf_rules_count\", \"accuracy\", \"std\"]]\n",
    "    datasets_df.to_excel(f\"Results/{dataset.name}-results.xlsx\", index=True)\n",
    "    return datasets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259edefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params(datasets):\n",
    "    dictionary = {\n",
    "        \"dataset_name\" : [],\n",
    "        \"best_accuracy\": [],\n",
    "        \"achieved_for\": [],\n",
    "        \"execution_time\": []\n",
    "    }\n",
    "\n",
    "    for dataset in datasets:\n",
    "        start_time = time.time()\n",
    "        tested_df = test_all_params(dataset)\n",
    "        end_time = time.time()\n",
    "        rows_with_best_acc = tested_df.loc[tested_df['accuracy'] == tested_df['accuracy'].max()]\n",
    "        rows_with_best_acc.to_excel(f\"Results/{dataset.name}-best-rows.xlsx\", index=True)\n",
    "        display(rows_with_best_acc)\n",
    "        \n",
    "        dictionary[\"dataset_name\"].append(dataset.name)\n",
    "        dictionary[\"best_accuracy\"].append(tested_df['accuracy'].max())\n",
    "        \n",
    "        entries = []\n",
    "        rows_with_best_acc.reset_index(inplace=True)\n",
    "        for ind in rows_with_best_acc.index:\n",
    "            entries.append(f\"s:{rows_with_best_acc.iloc[ind]['min_supp']}, c:{rows_with_best_acc.iloc[ind]['min_conf']};\")\n",
    "        dictionary[\"achieved_for\"].append(entries)\n",
    "        \n",
    "        dictionary[\"execution_time\"].append(end_time - start_time)\n",
    "\n",
    "    datasets_df = pd.DataFrame(dictionary)\n",
    "    datasets_df = datasets_df[[\"dataset_name\", \"best_accuracy\", \"achieved_for\", \"execution_time\"]]\n",
    "    datasets_df.to_excel(f\"Results/all-datasets-results.xlsx\", index=True)\n",
    "    display(datasets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c5055",
   "metadata": {},
   "source": [
    "<h1>Experimental part</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf1f09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    Dataset(name=\"cars\", path=\"datasets/mod_cars.csv\"),\n",
    "    Dataset(name=\"tic tac toe\", path=\"datasets/mod_tic_tac_toe.csv\"),\n",
    "    Dataset(name=\"balance-scale\", path=\"datasets/mod_balance_scale.csv\", target=\"Class\"),\n",
    "    Dataset(name=\"breast cancer\", path=\"datasets/mod_breast_cancer.csv\", na_values=\"?\", target=\"Class\"),\n",
    "    Dataset(name=\"nursery\", path=\"datasets/mod_nursery.csv\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e325661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing datasets with following config: Min_supp: 1%, Min_conf: 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>rows_count</th>\n",
       "      <th>att_count</th>\n",
       "      <th>rules_count</th>\n",
       "      <th>min_rule_len</th>\n",
       "      <th>avg_rule_len</th>\n",
       "      <th>avg_rule_len_attr_ratio</th>\n",
       "      <th>max_rule_len</th>\n",
       "      <th>min_rule_supp</th>\n",
       "      <th>avg_rule_supp</th>\n",
       "      <th>max_rule_supp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cars</td>\n",
       "      <td>1728</td>\n",
       "      <td>6</td>\n",
       "      <td>1164</td>\n",
       "      <td>1</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>4</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.96</td>\n",
       "      <td>70.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tic tac toe</td>\n",
       "      <td>958</td>\n",
       "      <td>9</td>\n",
       "      <td>8952</td>\n",
       "      <td>1</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.51</td>\n",
       "      <td>6</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.24</td>\n",
       "      <td>65.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>balance-scale</td>\n",
       "      <td>625</td>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.85</td>\n",
       "      <td>46.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>breast cancer</td>\n",
       "      <td>286</td>\n",
       "      <td>9</td>\n",
       "      <td>11520</td>\n",
       "      <td>1</td>\n",
       "      <td>5.31</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.42</td>\n",
       "      <td>70.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nursery</td>\n",
       "      <td>12960</td>\n",
       "      <td>8</td>\n",
       "      <td>3507</td>\n",
       "      <td>1</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.22</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset_name  rows_count  att_count  rules_count  min_rule_len  \\\n",
       "0           cars        1728          6         1164             1   \n",
       "1    tic tac toe         958          9         8952             1   \n",
       "2  balance-scale         625          4          250             1   \n",
       "3  breast cancer         286          9        11520             1   \n",
       "4        nursery       12960          8         3507             1   \n",
       "\n",
       "   avg_rule_len  avg_rule_len_attr_ratio  max_rule_len  min_rule_supp  \\\n",
       "0          3.67                     0.61             4           1.04   \n",
       "1          4.62                     0.51             6           1.04   \n",
       "2          2.84                     0.71             3           1.12   \n",
       "3          5.31                     0.59             9           1.05   \n",
       "4          3.89                     0.49             5           1.00   \n",
       "\n",
       "   avg_rule_supp  max_rule_supp  \n",
       "0           2.96          70.02  \n",
       "1           2.24          65.34  \n",
       "2           3.85          46.08  \n",
       "3           2.42          70.28  \n",
       "4           2.22          33.33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets_resume = process_data(datasets, 1, 20)\n",
    "datasets_resume.to_excel('Results/datasets_resume.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c9dd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing datasets with following config: Min_supp: 1%, Min_conf: 20%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#for estimating execution time per dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtest_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mtest_data\u001b[1;34m(datasets, min_sup, min_conf)\u001b[0m\n\u001b[0;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m k_fold_cross_validate(dataset, min_sup, min_conf)\n\u001b[1;32m---> 13\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     15\u001b[0m dictionary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(dataset\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "#for estimating execution time per dataset\n",
    "test_data(datasets, 1, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b7ea6",
   "metadata": {},
   "source": [
    "<h1>Finding best params per set</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c766a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_params(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
